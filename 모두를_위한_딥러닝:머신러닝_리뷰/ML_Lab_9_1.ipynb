{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 9-1 xor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic regression for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > 학습이 진행되지 않음\n",
    "* 10000 회 반복해도 cost가 많이 줄지 않음 (0.81 -> 0.69 )\n",
    "* logistic regression으로 해결이 안됨?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NN for Xor\n",
    "* 학습하는 레이어를 하나 더 추가해서 Neural Net 구성\n",
    "* 가중치(weight)와 편차(bias)값도 한 세트 더 추가되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > 학습이 진행됨\n",
    "* cost가 줄어드는 것이 보임 ( 0.80 -> 0.03 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wide NN for Xor\n",
    "* Hidden layer의 가중치 갯수를 늘림\n",
    "* 좀더 조밀한 신경망을 구축 할 수 있음\n",
    "* 텐서플로우 공식홈페이지 자료 참고 : https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "\n",
    "![widenn](https://www.tensorflow.org/images/wide_n_deep.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > 좀더 세세한 관계를 표현 할 수 있음\n",
    "* 여러 가중치들 사이에서 영향을 주는 파라메터를 세밀하게 표현할 수 있음\n",
    "* 최종 cost도 더 작게 나옴 (물론 실행때마다 최종cost가 바뀌는건 있음)\n",
    "* 10000회 반복의 최종 cost가 0.80->0.03 : 0.80 -> 0.007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep NN for XOR\n",
    "* 신경망의 층 수를 늘려 복잡한 관계를 찾아보고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > cost가 보다 더 빠르게 작아짐\n",
    "* 최종 cost 자체는 wide만 적용 했을때와 크게 차이나지 않음 (0.007 : 0.001)\n",
    "* 하지만 5000번째 학습을 진행 할 때의 cost를 보면 deep&wide쪽이 훨씬 작은 것을 볼 수 있음 (0.02 : 0.004)\n",
    "* 연산 수는 많지만 작은 반복으로 빠른 학습을 할 수 있지 않을까(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Wide&Deep NN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 기존 logistic regression을 이용한 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Deep & Wide NN 을 이용한 MNIST\n",
    "* 안됩니다!! \n",
    "* cost는 줄어드는데 정확도가 올라가지 않아요 ㅠ\n",
    "* 왜 안돼는지 고민좀 해볼게요 ㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 1.762915738\n",
      "Epoch: 0002 cost = 1.590815602\n",
      "Epoch: 0003 cost = 1.557484824\n",
      "Epoch: 0004 cost = 1.538971888\n",
      "Epoch: 0005 cost = 1.526428502\n",
      "Learning finished\n",
      "Accuracy:  0.9243\n",
      "Label:  [5]\n",
      "Prediction:  [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123b0fcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "#W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "#b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "#hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([256]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 5.3 10 장에 있는 아이...\n",
    "* 이놈도 에러남. \n",
    "* xavier 초기값을 못 물러오는건지 모르겟음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.782843656\n",
      "Epoch: 0002 cost = 0.350906003\n",
      "Epoch: 0003 cost = 0.292114596\n",
      "Epoch: 0004 cost = 0.268020527\n",
      "Epoch: 0005 cost = 0.253819797\n",
      "Epoch: 0006 cost = 0.243243815\n",
      "Epoch: 0007 cost = 0.234583393\n",
      "Epoch: 0008 cost = 0.227604771\n",
      "Epoch: 0009 cost = 0.221675498\n",
      "Epoch: 0010 cost = 0.216536791\n",
      "Epoch: 0011 cost = 0.211513652\n",
      "Epoch: 0012 cost = 0.206947614\n",
      "Epoch: 0013 cost = 0.203351988\n",
      "Epoch: 0014 cost = 0.199878023\n",
      "Epoch: 0015 cost = 0.196595781\n",
      "Learning Finished!\n",
      "Accuracy: 0.9378\n",
      "Label:  [6]\n",
      "Prediction:  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEX4gKMIPBEX4gaAIPxDU/wPGTSeR5q61uAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128179f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lab 10 MNIST and Deep learning\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([10]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[10, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L1, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
